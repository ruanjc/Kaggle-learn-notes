{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        #Intermediate Machine Learning\n",
    "\n",
    "#shape of data--(number of rows, number of columns)\n",
    "print(name_of_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                          #Missing values\n",
    "    \n",
    "#To know how many columns has missing data and how many entries is missing\n",
    "miss_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "#get the name of the colums with missin data\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "#Method 1 Drop columns with missing values\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis = 1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis = 1)\n",
    "#axis = 0, along columns\n",
    "#axis = 1, along rows\n",
    "\n",
    "#Method 2 Imputation--fill the missing value with some number\n",
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid)) #Imputation\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns#put back the column names\n",
    "\n",
    "#Method 3 Imputation and note if some value is missing in that column\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy() #make a copy so that the original data will not be changed\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_balid_plus[col + '_was_missing'] = X_valid_plus[col].isnill()\n",
    "    #making a new cloumn indicating what will be imputed\n",
    "#then repeat the process of imputation in method 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                          #Categorical Variables\n",
    "    \n",
    "#get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')#dtype = datatype\n",
    "object_cols = list(s[s].index)\n",
    "    \n",
    "#Method 1 Drop Categorical Variables\n",
    "drop_X_train = X_train.select_dtypes(exclude = ['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude = ['object'])\n",
    "\n",
    "#Method 2 Label encoding--assigns each value to a different integer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()#make copy to avoid chaging original data\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = encoder.transform(X_valid[col])]#apply label encder to each column with categprical data\n",
    "\n",
    "\n",
    "#Method 3 One-hot encoding--create new columns indicating the presence of each possible value in the proginal data\n",
    "#Method 3 works well when there is no clear order of categorical variable(no intrinic ranking--nominal vaiables)\n",
    "from sklearn.preprocessing import OnehotEncoder\n",
    "\n",
    "OH_encoder = OnehotEncoder(handle_unknown = 'ignore', sparse = False)\n",
    "#handle_unknown = 'ignore' avoid errors when the validation data contains classes that arent represented in the training data\n",
    "#sparse = False ensures that the encoded columns are returned as a numpy array\n",
    "OH_cols_train = pd.Dataframe(OH-encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.Dataframe(OH-encoder.traisform(X_valid[object_cols]))# apply one-hot encoder to each column with catagorical data\n",
    "\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index#put index back\n",
    "\n",
    "num_X_train = X_train.drop(object_cols, axis = 1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis = 1)#remove categorical columns\n",
    "\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis = 1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis = 1)#Add one-hot encoded columns to numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "\n",
    "\n",
    "#3 steps in total\n",
    "#Step 1\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEndoder\n",
    "#Preprocessing for nuerical data(missing value)\n",
    "numerical_transformer = SimpleImputer(strategy = 'constant')\n",
    "#Preprocessing for categorical data\n",
    "categorcial_transformer = pipeline(steps = [('imputer',SimpleImputer(strategy = 'most_frequent')),('onehot',OneHotEncoder(handle_unknown = 'ignore'))])\n",
    "\n",
    "#bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(transformers = [('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "#Step 2\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimator=100, random_state = 0)\n",
    "\n",
    "#Step 3\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps = [('preprocessor', preprocessor),('model', model)])\n",
    "#preprocessing of validation data, get predictions\n",
    "my_pipeline.fit(X_train, y_train)\n",
    "#preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "#Evaluate the model\n",
    "score = mean_absoluate_error(y_valid, preds)\n",
    "print('MAE:',score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
